

<!DOCTYPE html>
<html lang="zh" color-mode=light>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Internlm-03-基于 InternLM 和 LangChain 搭建你的知识库 - EnableAsync&#39;s Blog</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="google" content="notranslate" />
  <meta name="keywords" content="golang,java">
  <meta name="description" content="基于 InternLM 和 LangChain 搭建你...">
  <meta name="author" content="EnableAsync">
  <link rel="icon" href="/images/icons/favicon-16x16.png" type="image/png" sizes="16x16">
  <link rel="icon" href="/images/icons/favicon-32x32.png" type="image/png" sizes="32x32">
  <link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180">
  <meta rel="mask-icon" href="/images/icons/stun-logo.svg" color="#333333">
  
    <meta rel="msapplication-TileImage" content="/images/icons/favicon-144x144.png">
    <meta rel="msapplication-TileColor" content="#000000">
  

  
<link rel="stylesheet" href="/css/style.css">


  
    
<link rel="stylesheet" href="/lib/iconfont/iconfont.css">

  

  
    
<link rel="stylesheet" href="/lib/fancybox/fancybox.css">

  

  
    
    
<link rel="stylesheet" href="/lib/highlight/a11y-dark.css">

  

  <script>
    var CONFIG = window.CONFIG || {};
    var ZHAOO = window.ZHAOO || {};
    CONFIG = {
      isHome: false,
      fancybox: true,
      pjax: false,
      lazyload: {
        enable: true,
        only_post: 'false',
        loading: '[object Object]'
      },
      donate: {
        enable: false,
        alipay: 'https://pic.izhaoo.com/alipay.jpg',
        wechat: 'https://pic.izhaoo.com/wechat.jpg'
      },
      galleries: {
        enable: true
      },
      fab: {
        enable: true,
        always_show: false
      },
      carrier: {
        enable: false
      },
      daovoice: {
        enable: false
      },
      preview: {
        background: {
          default: '',
          api: ''
        },
        motto: {
          default: '生于忧患，死于安乐',
          typing: true,
          api: '',
          data_contents: ''
        },
      },
      qrcode: {
        enable: true,
        type: 'url',
        image: 'https://pic.izhaoo.com/weapp-code.jpg',
      },
      toc: {
        enable: true
      },
      scrollbar: {
        type: 'default'
      },
      notification: {
        enable: false,
        delay: 4500,
        list: '',
        page_white_list: '',
        page_black_list: ''
      },
      search: {
        enable: true,
        path: 'search.xml'
      }
    }
  </script>

  

  

<meta name="generator" content="Hexo 6.3.0"></head>

<body class="lock-screen">
  <div class="loading"></div>
  
    


  <nav class="navbar">
    <div class="left">
      
        <i class="iconfont iconhome j-navbar-back-home"></i>
      
      
        <i class="iconfont iconqrcode j-navbar-qrcode"></i>
      
      
        <i class="iconfont iconmoono" id="color-toggle" color-toggle="light"></i>
      
      
        <i class="iconfont iconsearch j-navbar-search"></i>
      
    </div>
    <div class="center">Internlm-03-基于 InternLM 和 LangChain 搭建你的知识库</div>
    <div class="right">
      <i class="iconfont iconmenu j-navbar-menu"></i>
    </div>
    
      <div id="qrcode-navbar"></div>
    
  </nav>

  
  

<nav class="menu">
  <div class="menu-container">
    <div class="menu-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <ul class="menu-content"><li class="menu-item">
        <a href="/ " class="underline "> 首页</a>
      </li><li class="menu-item">
        <a href="/categories/ " class="underline "> 分类</a>
      </li></ul>
    
      <div class="menu-copyright"><p>Powered by <a target="_blank" href="https://hexo.io">Hexo</a>  |  Theme - <a target="_blank" href="https://github.com/izhaoo/hexo-theme-zhaoo">zhaoo</a></p></div>
    
  </div>
</nav>
  <main id="main">
  <div class="article-wrap">
    <div class="row container">
      <div class="col-xl-3"></div>
      <div class="col-xl-6"><article class="article">
  <div class="wrap">
    <section class="head">
  <img   class="lazyload" data-original="/images/theme/post.jpg" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="  draggable="false">
  <div class="head-mask">
    <h1 class="head-title">Internlm-03-基于 InternLM 和 LangChain 搭建你的知识库</h1>
    <div class="head-info">
      <span class="post-info-item"><i class="iconfont iconcalendar"></i>January 10, 2024</span>
      
      <span class="post-info-item"><i class="iconfont iconfont-size"></i>17575</span>
    </div>
  </div>
</section>
    <section class="main">
      <section class="content">
        <h1 id="基于-InternLM-和-LangChain-搭建你的知识库"><a href="#基于-InternLM-和-LangChain-搭建你的知识库" class="headerlink" title="基于 InternLM 和 LangChain 搭建你的知识库"></a>基于 InternLM 和 LangChain 搭建你的知识库</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><h3 id="InternLM-环境"><a href="#InternLM-环境" class="headerlink" title="InternLM 环境"></a>InternLM 环境</h3><p>开发环境除了 <code>pytorch</code> 等库以外，还需要安装以下库</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 升级pip</span><br>python -m pip install --upgrade pip<br><br>pip install modelscope==1.9.5<br>pip install transformers==4.35.2<br>pip install streamlit==1.24.0<br>pip install sentencepiece==0.1.99<br>pip install accelerate==0.24.1<br></code></pre></td></tr></table></figure>
<h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">import torch<br>from modelscope import snapshot_download, AutoModel, AutoTokenizer<br>import os<br>model_dir = snapshot_download(<span class="hljs-string">&#x27;Shanghai_AI_Laboratory/internlm-chat-7b&#x27;</span>, cache_dir=<span class="hljs-string">&#x27;/root/data/model&#x27;</span>, revision=<span class="hljs-string">&#x27;v1.0.3&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="配置-Langchain"><a href="#配置-Langchain" class="headerlink" title="配置 Langchain"></a>配置 Langchain</h3><p>除了配置大模型的运行环境以外，还需要配置 Langchain 运行环境。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install langchain==0.0.292<br>pip install gradio==4.4.0<br>pip install chromadb==0.4.15<br>pip install sentence-transformers==2.2.2<br>pip install unstructured==0.10.30<br>pip install markdown==3.3.7<br></code></pre></td></tr></table></figure>
<p><img    class="lazyload" data-original="安装依赖.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">安装依赖</span></p>
<h3 id="下载-Embedding-模型"><a href="#下载-Embedding-模型" class="headerlink" title="下载 Embedding 模型"></a>下载 Embedding 模型</h3><p>同时，我们需要使用到开源词向量模型 <a target="_blank" rel="noopener" href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2">Sentence Transformer - paraphrase-multilingual-MiniLM-L12-v2</a>:（我们也可以选用别的开源词向量模型来进行 Embedding，教程中选用这个模型是相对轻量、支持中文且效果较好的，我这里选择使用了更为好用的 bge 系列的 Embedding 模型 <a href="[BAAI/bge-large-zh-v1.5 · Hugging Face](https://huggingface.co/BAAI/bge-large-zh-v1.5">BAAI/bge-large-zh-v1.5</a>)）</p>
<p>首先需要使用 <code>huggingface</code> 官方提供的 <code>huggingface-cli</code> 命令行工具。安装依赖:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install -U huggingface_hub<br></code></pre></td></tr></table></figure>
<p>然后在和 <code>/root/data</code> 目录下新建python文件 <code>download_hf.py</code>，填入以下代码：</p>
<ul>
<li>resume-download：断点续下</li>
<li>local-dir：本地存储路径。（linux环境下需要填写绝对路径）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 下载模型</span><br>os.system(<span class="hljs-string">&#x27;huggingface-cli download --resume-download BAAI/bge-large-zh-v1.5 --local-dir /root/data/model/bge-large-zh-v1.5&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p>但是，使用 huggingface 下载可能速度较慢，我们可以使用 huggingface 镜像下载。与使用hugginge face下载相同，只需要填入镜像地址即可。</p>
<p>将 <code>download_hf.py</code> 中的代码修改为以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 设置环境变量</span><br>os.environ[<span class="hljs-string">&#x27;HF_ENDPOINT&#x27;</span>] = <span class="hljs-string">&#x27;https://hf-mirror.com&#x27;</span><br><br><span class="hljs-comment"># 下载模型</span><br>os.system(<span class="hljs-string">&#x27;huggingface-cli download --resume-download BAAI/bge-large-zh-v1.5 --local-dir /root/data/model/bge-large-zh-v1.5&#x27;</span>)<br></code></pre></td></tr></table></figure>
<p>然后，在 <code>/root/data</code> 目录下执行该脚本即可自动开始下载：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python download_hf.py<br></code></pre></td></tr></table></figure>
<p><img    class="lazyload" data-original="下载bge.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">下载bge模型</span></p>
<h3 id="下载-NLTK-相关资源"><a href="#下载-NLTK-相关资源" class="headerlink" title="下载 NLTK 相关资源"></a>下载 NLTK 相关资源</h3><p>我们在使用开源词向量模型构建开源词向量的时候，需要用到第三方库 <code>nltk</code> 的一些资源。正常情况下，其会自动从互联网上下载，但可能由于网络原因会导致下载中断，此处我们可以从国内仓库镜像地址下载相关资源，保存到服务器上。</p>
<p>我们用以下命令下载 nltk 资源并解压到服务器上：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root<br>git <span class="hljs-built_in">clone</span> https://gitee.com/yzy0612/nltk_data.git  --branch gh-pages<br><span class="hljs-built_in">cd</span> nltk_data<br><span class="hljs-built_in">mv</span> packages/*  ./<br><span class="hljs-built_in">cd</span> tokenizers<br>unzip punkt.zip<br><span class="hljs-built_in">cd</span> ../taggers<br>unzip averaged_perceptron_tagger.zip<br></code></pre></td></tr></table></figure>
<p>之后使用时服务器即会自动使用已有资源，无需再次下载。</p>
<h3 id="下载教程代码"><a href="#下载教程代码" class="headerlink" title="下载教程代码"></a>下载教程代码</h3><p>我们在仓库中同步提供了所有脚本，可以查看该教程文件的同级目录的 <code>demo</code> 文件夹。</p>
<p>建议通过以下目录将仓库 clone 到本地，可以直接在本地运行相关代码：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">cd</span> /root/data<br>git <span class="hljs-built_in">clone</span> https://github.com/InternLM/tutorial<br></code></pre></td></tr></table></figure>
<p>通过上述命令，可以将本仓库 clone 到本地 <code>root/data/tutorial</code> 目录下，在之后的过程中可以对照仓库中的脚本来完成自己的代码，也可以直接使用仓库中的脚本。</p>
<h2 id="知识库搭建"><a href="#知识库搭建" class="headerlink" title="知识库搭建"></a>知识库搭建</h2><h3 id="数据收集"><a href="#数据收集" class="headerlink" title="数据收集"></a>数据收集</h3><p>教程选择了由上海人工智能实验室开源的一系列大模型工具开源仓库作为语料库来源，包括：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://gitee.com/open-compass/opencompass">OpenCompass</a>：面向大模型评测的一站式平台</li>
<li><a target="_blank" rel="noopener" href="https://gitee.com/InternLM/lmdeploy">IMDeploy</a>：涵盖了 LLM 任务的全套轻量化、部署和服务解决方案的高效推理工具箱</li>
<li><a target="_blank" rel="noopener" href="https://gitee.com/InternLM/xtuner">XTuner</a>：轻量级微调大语言模型的工具库</li>
<li><a target="_blank" rel="noopener" href="https://gitee.com/InternLM/InternLM-XComposer">InternLM-XComposer</a>：浦语·灵笔，基于书生·浦语大语言模型研发的视觉-语言大模型</li>
<li><a target="_blank" rel="noopener" href="https://gitee.com/InternLM/lagent">Lagent</a>：一个轻量级、开源的基于大语言模型的智能体（agent）框架</li>
<li><a target="_blank" rel="noopener" href="https://gitee.com/InternLM/InternLM">InternLM</a>：一个开源的轻量级训练框架，旨在支持大模型训练而无需大量的依赖</li>
</ul>
<p>首先我们需要将上述远程开源仓库 Clone 到本地，可以使用以下命令：</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-comment"># 进入到数据库盘</span><br>cd <span class="hljs-regexp">/root/</span>data<br><span class="hljs-comment"># clone 上述开源仓库</span><br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/open-compass/</span>opencompass.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>lmdeploy.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>xtuner.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>InternLM-XComposer.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>lagent.git<br>git clone https:<span class="hljs-regexp">//gi</span>tee.com<span class="hljs-regexp">/InternLM/</span>InternLM.git<br></code></pre></td></tr></table></figure>
<p>接着，为语料处理方便，我们将选用上述仓库中所有的 markdown、txt 文件作为示例语料库。注意，也可以选用其中的代码文件加入到知识库中，但需要针对代码文件格式进行额外处理（因为代码文件对逻辑联系要求较高，且规范性较强，在分割时最好基于代码模块进行分割再加入向量数据库）。</p>
<p>我们首先将上述仓库中所有满足条件的文件路径找出来，我们定义一个函数，该函数将递归指定文件夹路径，返回其中所有满足条件（即后缀名为 .md 或者 .txt 的文件）的文件路径：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_files</span>(<span class="hljs-params">dir_path</span>):<br>    <span class="hljs-comment"># args：dir_path，目标文件夹路径</span><br>    file_list = []<br>    <span class="hljs-keyword">for</span> filepath, dirnames, filenames <span class="hljs-keyword">in</span> os.walk(dir_path):<br>        <span class="hljs-comment"># os.walk 函数将递归遍历指定文件夹</span><br>        <span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> filenames:<br>            <span class="hljs-comment"># 通过后缀名判断文件类型是否满足要求</span><br>            <span class="hljs-keyword">if</span> filename.endswith(<span class="hljs-string">&quot;.md&quot;</span>):<br>                <span class="hljs-comment"># 如果满足要求，将其绝对路径加入到结果列表</span><br>                file_list.append(os.path.join(filepath, filename))<br>            <span class="hljs-keyword">elif</span> filename.endswith(<span class="hljs-string">&quot;.txt&quot;</span>):<br>                file_list.append(os.path.join(filepath, filename))<br>    <span class="hljs-keyword">return</span> file_list<br></code></pre></td></tr></table></figure>
<h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><p>得到所有目标文件路径之后，我们可以使用 LangChain 提供的 FileLoader 对象来加载目标文件，得到由目标文件解析出的纯文本内容。由于不同类型的文件需要对应不同的 FileLoader，我们判断目标文件类型，并针对性调用对应类型的 FileLoader，同时，调用 FileLoader 对象的 load 方法来得到加载之后的纯文本对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> UnstructuredFileLoader<br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> UnstructuredMarkdownLoader<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text</span>(<span class="hljs-params">dir_path</span>):<br>    <span class="hljs-comment"># args：dir_path，目标文件夹路径</span><br>    <span class="hljs-comment"># 首先调用上文定义的函数得到目标文件路径列表</span><br>    file_lst = get_files(dir_path)<br>    <span class="hljs-comment"># docs 存放加载之后的纯文本对象</span><br>    docs = []<br>    <span class="hljs-comment"># 遍历所有目标文件</span><br>    <span class="hljs-keyword">for</span> one_file <span class="hljs-keyword">in</span> tqdm(file_lst):<br>        file_type = one_file.split(<span class="hljs-string">&#x27;.&#x27;</span>)[-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> file_type == <span class="hljs-string">&#x27;md&#x27;</span>:<br>            loader = UnstructuredMarkdownLoader(one_file)<br>        <span class="hljs-keyword">elif</span> file_type == <span class="hljs-string">&#x27;txt&#x27;</span>:<br>            loader = UnstructuredFileLoader(one_file)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 如果是不符合条件的文件，直接跳过</span><br>            <span class="hljs-keyword">continue</span><br>        docs.extend(loader.load())<br>    <span class="hljs-keyword">return</span> docs<br></code></pre></td></tr></table></figure>
<p>使用上文函数，我们得到的 <code>docs</code> 为一个纯文本对象对应的列表。</p>
<h3 id="构建向量数据库"><a href="#构建向量数据库" class="headerlink" title="构建向量数据库"></a>构建向量数据库</h3><p>得到该列表之后，我们就可以将它引入到 LangChain 框架中构建向量数据库。由纯文本对象构建向量数据库，我们需要先对文本进行分块，接着对文本块进行向量化。</p>
<p>LangChain 提供了多种文本分块工具，此处我们使用字符串递归分割器，并选择分块大小为 500，块重叠长度为 150（由于篇幅限制，此处没有展示切割效果，学习者可以自行尝试一下，想要深入学习 LangChain 文本分块可以参考教程 <a target="_blank" rel="noopener" href="https://github.com/datawhalechina/prompt-engineering-for-developers/blob/9dbcb48416eb8af9ff9447388838521dc0f9acb0/content/LangChain Chat with Your Data/1.简介 Introduction.md">《LangChain - Chat With Your Data》</a>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter<br><br>text_splitter = RecursiveCharacterTextSplitter(<br>    chunk_size=<span class="hljs-number">500</span>, chunk_overlap=<span class="hljs-number">150</span>)<br>split_docs = text_splitter.split_documents(docs)<br></code></pre></td></tr></table></figure>
<p>接着我们选用开源词向量模型 <a target="_blank" rel="noopener" href="https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2">Sentence Transformer</a> 来进行文本向量化。LangChain 提供了直接引入 HuggingFace 开源社区中的模型进行向量化的接口：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings<br><br>embeddings = HuggingFaceEmbeddings(model_name=<span class="hljs-string">&quot;/root/data/model/bge-large-zh-v1.5&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>同时，考虑到 Chroma 是目前最常用的入门数据库，我们选择 Chroma 作为向量数据库，基于上文分块后的文档以及加载的开源向量化模型，将语料加载到指定路径下的向量数据库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><br><span class="hljs-comment"># 定义持久化路径</span><br>persist_directory = <span class="hljs-string">&#x27;data_base/vector_db/chroma&#x27;</span><br><span class="hljs-comment"># 加载数据库</span><br>vectordb = Chroma.from_documents(<br>    documents=split_docs,<br>    embedding=embeddings,<br>    persist_directory=persist_directory  <span class="hljs-comment"># 允许我们将persist_directory目录保存到磁盘上</span><br>)<br><span class="hljs-comment"># 将加载的向量数据库持久化到磁盘上</span><br>vectordb.persist()<br></code></pre></td></tr></table></figure>
<h3 id="整体脚本"><a href="#整体脚本" class="headerlink" title="整体脚本"></a>整体脚本</h3><p>将上述代码整合在一起为知识库搭建的脚本：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 首先导入所需第三方库</span><br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> UnstructuredFileLoader<br><span class="hljs-keyword">from</span> langchain.document_loaders <span class="hljs-keyword">import</span> UnstructuredMarkdownLoader<br><span class="hljs-keyword">from</span> langchain.text_splitter <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter<br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 获取文件路径函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_files</span>(<span class="hljs-params">dir_path</span>):<br>    <span class="hljs-comment"># args：dir_path，目标文件夹路径</span><br>    file_list = []<br>    <span class="hljs-keyword">for</span> filepath, dirnames, filenames <span class="hljs-keyword">in</span> os.walk(dir_path):<br>        <span class="hljs-comment"># os.walk 函数将递归遍历指定文件夹</span><br>        <span class="hljs-keyword">for</span> filename <span class="hljs-keyword">in</span> filenames:<br>            <span class="hljs-comment"># 通过后缀名判断文件类型是否满足要求</span><br>            <span class="hljs-keyword">if</span> filename.endswith(<span class="hljs-string">&quot;.md&quot;</span>):<br>                <span class="hljs-comment"># 如果满足要求，将其绝对路径加入到结果列表</span><br>                file_list.append(os.path.join(filepath, filename))<br>            <span class="hljs-keyword">elif</span> filename.endswith(<span class="hljs-string">&quot;.txt&quot;</span>):<br>                file_list.append(os.path.join(filepath, filename))<br>    <span class="hljs-keyword">return</span> file_list<br><br><span class="hljs-comment"># 加载文件函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_text</span>(<span class="hljs-params">dir_path</span>):<br>    <span class="hljs-comment"># args：dir_path，目标文件夹路径</span><br>    <span class="hljs-comment"># 首先调用上文定义的函数得到目标文件路径列表</span><br>    file_lst = get_files(dir_path)<br>    <span class="hljs-comment"># docs 存放加载之后的纯文本对象</span><br>    docs = []<br>    <span class="hljs-comment"># 遍历所有目标文件</span><br>    <span class="hljs-keyword">for</span> one_file <span class="hljs-keyword">in</span> tqdm(file_lst):<br>        file_type = one_file.split(<span class="hljs-string">&#x27;.&#x27;</span>)[-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> file_type == <span class="hljs-string">&#x27;md&#x27;</span>:<br>            loader = UnstructuredMarkdownLoader(one_file)<br>        <span class="hljs-keyword">elif</span> file_type == <span class="hljs-string">&#x27;txt&#x27;</span>:<br>            loader = UnstructuredFileLoader(one_file)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 如果是不符合条件的文件，直接跳过</span><br>            <span class="hljs-keyword">continue</span><br>        docs.extend(loader.load())<br>    <span class="hljs-keyword">return</span> docs<br><br><span class="hljs-comment"># 目标文件夹</span><br>tar_dir = [<br>    <span class="hljs-string">&quot;/root/data/InternLM&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/InternLM-XComposer&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/lagent&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/lmdeploy&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/opencompass&quot;</span>,<br>    <span class="hljs-string">&quot;/root/data/xtuner&quot;</span><br>]<br><br><span class="hljs-comment"># 加载目标文件</span><br>docs = []<br><span class="hljs-keyword">for</span> dir_path <span class="hljs-keyword">in</span> tar_dir:<br>    docs.extend(get_text(dir_path))<br><br><span class="hljs-comment"># 对文本进行分块</span><br>text_splitter = RecursiveCharacterTextSplitter(<br>    chunk_size=<span class="hljs-number">500</span>, chunk_overlap=<span class="hljs-number">150</span>)<br>split_docs = text_splitter.split_documents(docs)<br><br><span class="hljs-comment"># 加载开源词向量模型</span><br>embeddings = HuggingFaceEmbeddings(model_name=<span class="hljs-string">&quot;/root/data/model/bge-large-zh-v1.5&quot;</span>)<br><br><span class="hljs-comment"># 构建向量数据库</span><br><span class="hljs-comment"># 定义持久化路径</span><br>persist_directory = <span class="hljs-string">&#x27;data_base/vector_db/chroma&#x27;</span><br><span class="hljs-comment"># 加载数据库</span><br>vectordb = Chroma.from_documents(<br>    documents=split_docs,<br>    embedding=embeddings,<br>    persist_directory=persist_directory  <span class="hljs-comment"># 允许我们将persist_directory目录保存到磁盘上</span><br>)<br><span class="hljs-comment"># 将加载的向量数据库持久化到磁盘上</span><br>vectordb.persist()<br></code></pre></td></tr></table></figure>
<p>可以在 <code>/root/data</code> 下新建一个 <code>demo</code>目录，将该脚本和后续脚本均放在该目录下运行。运行上述脚本，即可在本地构建已持久化的向量数据库，后续直接导入该数据库即可，无需重复构建。</p>
<h2 id="InternLM-接入-LangChain"><a href="#InternLM-接入-LangChain" class="headerlink" title="InternLM 接入 LangChain"></a>InternLM 接入 LangChain</h2><p>为便捷构建 LLM 应用，我们需要基于本地部署的 InternLM，继承 LangChain 的 LLM 类自定义一个 InternLM LLM 子类，从而实现将 InternLM 接入到 LangChain 框架中。完成 LangChain 的自定义 LLM 子类之后，可以以完全一致的方式调用 LangChain 的接口，而无需考虑底层模型调用的不一致。</p>
<p>基于本地部署的 InternLM 自定义 LLM 类并不复杂，我们只需从 LangChain.llms.base.LLM 类继承一个子类，并重写构造函数与 <code>_call</code> 函数即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.llms.base <span class="hljs-keyword">import</span> LLM<br><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Any</span>, <span class="hljs-type">List</span>, <span class="hljs-type">Optional</span><br><span class="hljs-keyword">from</span> langchain.callbacks.manager <span class="hljs-keyword">import</span> CallbackManagerForLLMRun<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM<br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InternLM_LLM</span>(<span class="hljs-title class_ inherited__">LLM</span>):<br>    <span class="hljs-comment"># 基于本地 InternLM 自定义 LLM 类</span><br>    tokenizer : AutoTokenizer = <span class="hljs-literal">None</span><br>    model: AutoModelForCausalLM = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_path :<span class="hljs-built_in">str</span></span>):<br>        <span class="hljs-comment"># model_path: InternLM 模型路径</span><br>        <span class="hljs-comment"># 从本地初始化模型</span><br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;正在从本地加载模型...&quot;</span>)<br>        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=<span class="hljs-literal">True</span>)<br>        self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=<span class="hljs-literal">True</span>).to(torch.bfloat16).cuda()<br>        self.model = self.model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;完成本地模型的加载&quot;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_call</span>(<span class="hljs-params">self, prompt : <span class="hljs-built_in">str</span>, stop: <span class="hljs-type">Optional</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                run_manager: <span class="hljs-type">Optional</span>[CallbackManagerForLLMRun] = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                **kwargs: <span class="hljs-type">Any</span></span>):<br>        <span class="hljs-comment"># 重写调用函数</span><br>        system_prompt = <span class="hljs-string">&quot;&quot;&quot;You are an AI assistant whose name is InternLM (书生·浦语).</span><br><span class="hljs-string">        - InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.</span><br><span class="hljs-string">        - InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <br>        messages = [(system_prompt, <span class="hljs-string">&#x27;&#x27;</span>)]<br>        response, history = self.model.chat(self.tokenizer, prompt , history=messages)<br>        <span class="hljs-keyword">return</span> response<br>        <br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_llm_type</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;InternLM&quot;</span><br></code></pre></td></tr></table></figure>
<p>在上述类定义中，重写了构造函数和 <code>_call</code> 函数：对于构造函数，我们在对象实例化的一开始加载本地部署的 InternLM 模型，从而避免每一次调用都需要重新加载模型带来的时间过长；<code>_call</code> 函数是 LLM 类的核心函数，LangChain 会调用该函数来调用 LLM，在该函数中，我们调用已实例化模型的 chat 方法，从而实现对模型的调用并返回调用结果。</p>
<p>在整体项目中，我们将上述代码封装为 LLM.py，后续将直接从该文件中引入自定义的 LLM 类。</p>
<h2 id="构建检索问答链"><a href="#构建检索问答链" class="headerlink" title="构建检索问答链"></a>构建检索问答链</h2><p>LangChain 通过提供检索问答链对象来实现对于 RAG 全流程的封装。所谓检索问答链，即通过一个对象完成检索增强问答（即RAG）的全流程，针对 RAG 的更多概念，我们会在视频内容中讲解，也欢迎读者查阅该教程来进一步了解：<a target="_blank" rel="noopener" href="https://github.com/datawhalechina/llm-universe/tree/main">《LLM Universe》</a>。我们可以调用一个 LangChain 提供的 <code>RetrievalQA</code> 对象，通过初始化时填入已构建的数据库和自定义 LLM 作为参数，来简便地完成检索增强问答的全流程，LangChain 会自动完成基于用户提问进行检索、获取相关文档、拼接为合适的 Prompt 并交给 LLM 问答的全部流程。</p>
<h3 id="加载向量数据库"><a href="#加载向量数据库" class="headerlink" title="加载向量数据库"></a>加载向量数据库</h3><p>首先我们需要将上文构建的向量数据库导入进来，我们可以直接通过 Chroma 以及上文定义的词向量模型来加载已构建的数据库：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-comment"># 定义 Embeddings</span><br>embeddings = HuggingFaceEmbeddings(model_name=<span class="hljs-string">&quot;/root/data/model/bge-large-zh-v1.5&quot;</span>)<br><br><span class="hljs-comment"># 向量数据库持久化路径</span><br>persist_directory = <span class="hljs-string">&#x27;data_base/vector_db/chroma&#x27;</span><br><br><span class="hljs-comment"># 加载数据库</span><br>vectordb = Chroma(<br>    persist_directory=persist_directory, <br>    embedding_function=embeddings<br>)<br></code></pre></td></tr></table></figure>
<p>上述代码得到的 <code>vectordb</code> 对象即为我们已构建的向量数据库对象，该对象可以针对用户的 <code>query</code> 进行语义向量检索，得到与用户提问相关的知识片段。</p>
<h3 id="实例化自定义-LLM-与-Prompt-Template"><a href="#实例化自定义-LLM-与-Prompt-Template" class="headerlink" title="实例化自定义 LLM 与 Prompt Template"></a>实例化自定义 LLM 与 Prompt Template</h3><p>接着，我们实例化一个基于 InternLM 自定义的 LLM 对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> LLM <span class="hljs-keyword">import</span> InternLM_LLM<br>llm = InternLM_LLM(model_path = <span class="hljs-string">&quot;/root/data/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;</span>)<br>llm.predict(<span class="hljs-string">&quot;你是谁&quot;</span>)<br></code></pre></td></tr></table></figure>
<p>构建检索问答链，还需要构建一个 Prompt Template，该 Template 其实基于一个带变量的字符串，在检索之后，LangChain 会将检索到的相关文档片段填入到 Template 的变量中，从而实现带知识的 Prompt 构建。我们可以基于 LangChain 的 Template 基类来实例化这样一个 Template 对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate<br><br><span class="hljs-comment"># 我们所构造的 Prompt 模板</span><br>template = <span class="hljs-string">&quot;&quot;&quot;使用以下上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。</span><br><span class="hljs-string">问题: &#123;question&#125;</span><br><span class="hljs-string">可参考的上下文：</span><br><span class="hljs-string">···</span><br><span class="hljs-string">&#123;context&#125;</span><br><span class="hljs-string">···</span><br><span class="hljs-string">如果给定的上下文无法让你做出回答，请回答你不知道。</span><br><span class="hljs-string">有用的回答:&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># 调用 LangChain 的方法来实例化一个 Template 对象，该对象包含了 context 和 question 两个变量，在实际调用时，这两个变量会被检索到的文档片段和用户提问填充</span><br>QA_CHAIN_PROMPT = PromptTemplate(input_variables=[<span class="hljs-string">&quot;context&quot;</span>,<span class="hljs-string">&quot;question&quot;</span>],template=template)<br></code></pre></td></tr></table></figure>
<h3 id="构建检索问答链-1"><a href="#构建检索问答链-1" class="headerlink" title="构建检索问答链"></a>构建检索问答链</h3><p>最后，可以调用 LangChain 提供的检索问答链构造函数，基于我们的自定义 LLM、Prompt Template 和向量知识库来构建一个基于 InternLM 的检索问答链：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQA<br><br>qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectordb.as_retriever(),return_source_documents=<span class="hljs-literal">True</span>,chain_type_kwargs=&#123;<span class="hljs-string">&quot;prompt&quot;</span>:QA_CHAIN_PROMPT&#125;)<br></code></pre></td></tr></table></figure>
<p>得到的 <code>qa_chain</code> 对象即可以实现我们的核心功能，即基于 InternLM 模型的专业知识库助手。我们可以对比该检索问答链和纯 LLM 的问答效果：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 检索问答链回答效果</span><br>question = <span class="hljs-string">&quot;什么是InternLM&quot;</span><br>result = qa_chain(&#123;<span class="hljs-string">&quot;query&quot;</span>: question&#125;)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;检索问答链回答 question 的结果：&quot;</span>)<br><span class="hljs-built_in">print</span>(result[<span class="hljs-string">&quot;result&quot;</span>])<br><br><span class="hljs-comment"># 仅 LLM 回答效果</span><br>result_2 = llm(question)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;大模型回答 question 的结果：&quot;</span>)<br><span class="hljs-built_in">print</span>(result_2)<br></code></pre></td></tr></table></figure>
<h2 id="部署一个-Web-Demo"><a href="#部署一个-Web-Demo" class="headerlink" title="部署一个 Web Demo"></a>部署一个 Web Demo</h2><p>之后我们可以基于 Gradio 框架将其部署到 Web 网页，从而搭建一个小型 Demo，便于测试与使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入必要的库</span><br><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><span class="hljs-keyword">from</span> langchain.vectorstores <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain.embeddings.huggingface <span class="hljs-keyword">import</span> HuggingFaceEmbeddings<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> LLM <span class="hljs-keyword">import</span> InternLM_LLM<br><span class="hljs-keyword">from</span> langchain.prompts <span class="hljs-keyword">import</span> PromptTemplate<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_chain</span>():<br>    <span class="hljs-comment"># 加载问答链</span><br>    <span class="hljs-comment"># 定义 Embeddings</span><br>    embeddings = HuggingFaceEmbeddings(model_name=<span class="hljs-string">&quot;/root/data/model/bge-large-zh-v1.5&quot;</span>)<br><br>    <span class="hljs-comment"># 向量数据库持久化路径</span><br>    persist_directory = <span class="hljs-string">&#x27;data_base/vector_db/chroma&#x27;</span><br><br>    <span class="hljs-comment"># 加载数据库</span><br>    vectordb = Chroma(<br>        persist_directory=persist_directory,  <span class="hljs-comment"># 允许我们将persist_directory目录保存到磁盘上</span><br>        embedding_function=embeddings<br>    )<br><br>    llm = InternLM_LLM(model_path = <span class="hljs-string">&quot;/root/data/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;</span>)<br><br>    template = <span class="hljs-string">&quot;&quot;&quot;使用以下上下文来回答用户的问题。如果你不知道答案，就说你不知道。总是使用中文回答。</span><br><span class="hljs-string">    问题: &#123;question&#125;</span><br><span class="hljs-string">    可参考的上下文：</span><br><span class="hljs-string">    ···</span><br><span class="hljs-string">    &#123;context&#125;</span><br><span class="hljs-string">    ···</span><br><span class="hljs-string">    如果给定的上下文无法让你做出回答，请回答你不知道。</span><br><span class="hljs-string">    有用的回答:&quot;&quot;&quot;</span><br><br>    QA_CHAIN_PROMPT = PromptTemplate(input_variables=[<span class="hljs-string">&quot;context&quot;</span>,<span class="hljs-string">&quot;question&quot;</span>],<br>                                    template=template)<br><br>    <span class="hljs-comment"># 运行 chain</span><br>    <span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> RetrievalQA<br><br>    qa_chain = RetrievalQA.from_chain_type(llm,<br>                                        retriever=vectordb.as_retriever(),<br>                                        return_source_documents=<span class="hljs-literal">True</span>,<br>                                        chain_type_kwargs=&#123;<span class="hljs-string">&quot;prompt&quot;</span>:QA_CHAIN_PROMPT&#125;)<br>    <br>    <span class="hljs-keyword">return</span> qa_chain<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_center</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    存储问答 Chain 的对象 </span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.chain = load_chain()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">qa_chain_self_answer</span>(<span class="hljs-params">self, question: <span class="hljs-built_in">str</span>, chat_history: <span class="hljs-built_in">list</span> = []</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        调用不带历史记录的问答链进行回答</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> question == <span class="hljs-literal">None</span> <span class="hljs-keyword">or</span> <span class="hljs-built_in">len</span>(question) &lt; <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, chat_history<br>        <span class="hljs-keyword">try</span>:<br>            chat_history.append(<br>                (question, self.chain(&#123;<span class="hljs-string">&quot;query&quot;</span>: question&#125;)[<span class="hljs-string">&quot;result&quot;</span>]))<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>, chat_history<br>        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:<br>            <span class="hljs-keyword">return</span> e, chat_history<br><br><br>model_center = Model_center()<br><br>block = gr.Blocks()<br><span class="hljs-keyword">with</span> block <span class="hljs-keyword">as</span> demo:<br>    <span class="hljs-keyword">with</span> gr.Row(equal_height=<span class="hljs-literal">True</span>):   <br>        <span class="hljs-keyword">with</span> gr.Column(scale=<span class="hljs-number">15</span>):<br>            gr.Markdown(<span class="hljs-string">&quot;&quot;&quot;&lt;h1&gt;&lt;center&gt;InternLM&lt;/center&gt;&lt;/h1&gt;</span><br><span class="hljs-string">                &lt;center&gt;书生浦语&lt;/center&gt;</span><br><span class="hljs-string">                &quot;&quot;&quot;</span>)<br>        <span class="hljs-comment"># gr.Image(value=LOGO_PATH, scale=1, min_width=10,show_label=False, show_download_button=False)</span><br><br>    <span class="hljs-keyword">with</span> gr.Row():<br>        <span class="hljs-keyword">with</span> gr.Column(scale=<span class="hljs-number">4</span>):<br>            chatbot = gr.Chatbot(height=<span class="hljs-number">450</span>, show_copy_button=<span class="hljs-literal">True</span>)<br>            <span class="hljs-comment"># 创建一个文本框组件，用于输入 prompt。</span><br>            msg = gr.Textbox(label=<span class="hljs-string">&quot;Prompt/问题&quot;</span>)<br><br>            <span class="hljs-keyword">with</span> gr.Row():<br>                <span class="hljs-comment"># 创建提交按钮。</span><br>                db_wo_his_btn = gr.Button(<span class="hljs-string">&quot;Chat&quot;</span>)<br>            <span class="hljs-keyword">with</span> gr.Row():<br>                <span class="hljs-comment"># 创建一个清除按钮，用于清除聊天机器人组件的内容。</span><br>                clear = gr.ClearButton(<br>                    components=[chatbot], value=<span class="hljs-string">&quot;Clear console&quot;</span>)<br>                <br>        <span class="hljs-comment"># 设置按钮的点击事件。当点击时，调用上面定义的 qa_chain_self_answer 函数，并传入用户的消息和聊天历史记录，然后更新文本框和聊天机器人组件。</span><br>        db_wo_his_btn.click(model_center.qa_chain_self_answer, inputs=[<br>                            msg, chatbot], outputs=[msg, chatbot])<br>        <br>    gr.Markdown(<span class="hljs-string">&quot;&quot;&quot;提醒：&lt;br&gt;</span><br><span class="hljs-string">    1. 初始化数据库时间可能较长，请耐心等待。</span><br><span class="hljs-string">    2. 使用中如果出现异常，将会在文本输入框进行展示，请不要惊慌。 &lt;br&gt;</span><br><span class="hljs-string">    &quot;&quot;&quot;</span>)<br><span class="hljs-comment"># threads to consume the request</span><br>gr.close_all()<br><span class="hljs-comment"># 启动新的 Gradio 应用，设置分享功能为 True，并使用环境变量 PORT1 指定服务器端口。</span><br><span class="hljs-comment"># demo.launch(share=True, server_port=int(os.environ[&#x27;PORT1&#x27;]))</span><br><span class="hljs-comment"># 直接启动</span><br>demo.launch()<br></code></pre></td></tr></table></figure>
<p>运行截图如下：</p>
<p><img    class="lazyload" data-original="gradio.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">运行gradio</span></p>
<p><img    class="lazyload" data-original="Langchain+InternLM问答.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">Langchain+InternLM问答</span></p>
<p>如图，能够正确地回答知识库中的知识。</p>
<h2 id="问题解决以及-Langchain-调试"><a href="#问题解决以及-Langchain-调试" class="headerlink" title="问题解决以及 Langchain 调试"></a>问题解决以及 Langchain 调试</h2><p>我们在遇到奇怪问题的时候，想要调试 Langchain，这个时候可以借助 Langchain 的全局设置设置调试模式，设置方式如下所示：</p>
<p><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/guides/debugging">Debugging | 🦜️🔗 Langchain</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.<span class="hljs-built_in">globals</span> <span class="hljs-keyword">import</span> set_verbose <span class="hljs-comment"># 我这里用的 langchain 版本为 0.1.0</span><br><br>set_verbose(<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>
<p><img    class="lazyload" data-original="langchain-debug.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">langchain 的调试输出</span></p>
<h2 id="将应用部署在-OpenXLab-上"><a href="#将应用部署在-OpenXLab-上" class="headerlink" title="将应用部署在 OpenXLab 上"></a>将应用部署在 OpenXLab 上</h2><p><a target="_blank" rel="noopener" href="https://openxlab.org.cn/apps/detail/EnableAsync/network-bot">计算机网络问答机器人</a></p>
<h3 id="Sqlite-问题1"><a href="#Sqlite-问题1" class="headerlink" title="Sqlite 问题1"></a>Sqlite 问题<sup><a href="#fn_1" id="reffn_1">1</a></sup></h3><p>OpenXLab 上的 sqlite3 版本低于我们项目用的 Chroma 要求。可参考<a href="https://link.zhihu.com/?target=https%3A//docs.trychroma.com/troubleshooting%23sqlite"> Troubleshooting | Chroma (trychroma.com)</a>，在 <code>requirements.txt</code> 中添加 <code>pysqlite3-binary</code> ，之后加载 sqlite3 库来绕过这个问题。否则就要写脚本在运行时自己安装上更新版本的sqlite3了。下面是修改加载 sqlite3 库的 trick 命令：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">__import__</span>(<span class="hljs-string">&#x27;pysqlite3&#x27;</span>)<br><span class="hljs-keyword">import</span> sys<br>sys.modules[<span class="hljs-string">&#x27;sqlite3&#x27;</span>] = sys.modules.pop(<span class="hljs-string">&#x27;pysqlite3&#x27;</span>)<br></code></pre></td></tr></table></figure>
<h3 id="运行截图"><a href="#运行截图" class="headerlink" title="运行截图"></a>运行截图</h3><p><img    class="lazyload" data-original="openxlab-deploy.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">openxlab-deploy</span></p>
<p><img    class="lazyload" data-original="loading.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">加载模型</span></p>
<p><img    class="lazyload" data-original="部署.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">部署</span></p>
<p><img    class="lazyload" data-original="运行日志.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">运行日志</span></p>
<h2 id="参考内容"><a href="#参考内容" class="headerlink" title="参考内容"></a>参考内容</h2><blockquote id="fn_1">
<sup>1</sup>. <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/676719586">书生・浦语大模型实战营第三课作业(基础+进阶) - 知乎 (zhihu.com)</a><a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>

      </section>
      <section class="extra">
        
          <ul class="copyright">
  
    <li><strong>本文作者：</strong>EnableAsync</li>
    <li><strong>本文链接：</strong><a href="https://enableasync.github.io/internlm/internlm-03/index.html" title="https:&#x2F;&#x2F;enableasync.github.io&#x2F;internlm&#x2F;internlm-03&#x2F;index.html">https:&#x2F;&#x2F;enableasync.github.io&#x2F;internlm&#x2F;internlm-03&#x2F;index.html</a></li>
    <li><strong>版权声明：</strong>本博客所有文章均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" title="BY-NC-SA" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请注明出处！</li>
  
</ul>
        
        
        
        
  <nav class="nav">
    <a href="/internlm/internlm-04/"><i class="iconfont iconleft"></i>Internlm-04-XTuner 大模型单卡低成本微调实战</a>
    <a href="/internlm/internlm-02/">Internlm-02-浦语大模型趣味 Demo<i class="iconfont iconright"></i></a>
  </nav>

      </section>
      
    </section>
  </div>
</article></div>
      <div class="col-xl-3">
        
          
  <aside class="toc-wrap">
    <h3 class="toc-title">文章目录：</h3>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E-InternLM-%E5%92%8C-LangChain-%E6%90%AD%E5%BB%BA%E4%BD%A0%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93"><span class="toc-text">基于 InternLM 和 LangChain 搭建你的知识库</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-text">环境配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#InternLM-%E7%8E%AF%E5%A2%83"><span class="toc-text">InternLM 环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD"><span class="toc-text">模型下载</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE-Langchain"><span class="toc-text">配置 Langchain</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD-Embedding-%E6%A8%A1%E5%9E%8B"><span class="toc-text">下载 Embedding 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD-NLTK-%E7%9B%B8%E5%85%B3%E8%B5%84%E6%BA%90"><span class="toc-text">下载 NLTK 相关资源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E6%95%99%E7%A8%8B%E4%BB%A3%E7%A0%81"><span class="toc-text">下载教程代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E5%BA%93%E6%90%AD%E5%BB%BA"><span class="toc-text">知识库搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86"><span class="toc-text">数据收集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-text">加载数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">构建向量数据库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E8%84%9A%E6%9C%AC"><span class="toc-text">整体脚本</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#InternLM-%E6%8E%A5%E5%85%A5-LangChain"><span class="toc-text">InternLM 接入 LangChain</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE"><span class="toc-text">构建检索问答链</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">加载向量数据库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B%E5%8C%96%E8%87%AA%E5%AE%9A%E4%B9%89-LLM-%E4%B8%8E-Prompt-Template"><span class="toc-text">实例化自定义 LLM 与 Prompt Template</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%84%E5%BB%BA%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE-1"><span class="toc-text">构建检索问答链</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA-Web-Demo"><span class="toc-text">部署一个 Web Demo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E4%BB%A5%E5%8F%8A-Langchain-%E8%B0%83%E8%AF%95"><span class="toc-text">问题解决以及 Langchain 调试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%86%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2%E5%9C%A8-OpenXLab-%E4%B8%8A"><span class="toc-text">将应用部署在 OpenXLab 上</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sqlite-%E9%97%AE%E9%A2%981"><span class="toc-text">Sqlite 问题1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E6%88%AA%E5%9B%BE"><span class="toc-text">运行截图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E5%86%85%E5%AE%B9"><span class="toc-text">参考内容</span></a></li></ol></li></ol>
  </aside>

        
      </div>
    </div>
  </div>
</main>
  

<footer class="footer">
  <div class="footer-social"><a 
        href="https://github.com/EnableAsync "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#9f7be1'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  icongithub-fill "></i>
      </a></div>
  
    <div class="footer-copyright"><p>Powered by <a target="_blank" href="https://hexo.io">Hexo</a>  |  Theme - <a target="_blank" href="https://github.com/izhaoo/hexo-theme-zhaoo">zhaoo</a></p></div>
  
</footer>
  
      <div class="fab fab-plus">
    <i class="iconfont iconplus"></i>
  </div>
  
  
  <div class="fab fab-up">
    <i class="iconfont iconcaret-up"></i>
  </div>
  
  
  
    
<script src="/js/color-mode.js"></script>

  
  
    <div class="search">
  <div class="search-container">
    <div class="search-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <div class="search-input-wrapper">
      <i class="search-input-icon iconfont iconsearch"></i>
      <input class="search-input" type="search" id="search-input" placeholder="Search..." autofocus autocomplete="off"
        autocorrect="off" autocapitalize="off">
    </div>
    <div class="search-output" id="search-output"></div>
  </div>
</div>
  
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>

<script src="/lib/jquery/jquery.js"></script>



  
<script src="/lib/lazyload/lazyload.js"></script>




  
<script src="/lib/fancybox/fancybox.js"></script>






  
<script src="/lib/qrcode/qrcode.js"></script>




<script src="/js/utils.js"></script>
<script src="/js/script.js"></script>



















</html>