

<!DOCTYPE html>
<html lang="zh" color-mode=light>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>2025 年的 Diffusion 在研究什么 - EnableAsync&#39;s Blog</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="google" content="notranslate" />
  <meta name="keywords" content="golang,java">
  <meta name="description" content="好的，我将调研2024-2025年间关于扩散模型（Di...">
  <meta name="author" content="EnableAsync">
  <link rel="icon" href="/images/icons/favicon-16x16.png" type="image/png" sizes="16x16">
  <link rel="icon" href="/images/icons/favicon-32x32.png" type="image/png" sizes="32x32">
  <link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180">
  <meta rel="mask-icon" href="/images/icons/stun-logo.svg" color="#333333">
  
    <meta rel="msapplication-TileImage" content="/images/icons/favicon-144x144.png">
    <meta rel="msapplication-TileColor" content="#000000">
  

  
<link rel="stylesheet" href="/css/style.css">


  
    
<link rel="stylesheet" href="/lib/iconfont/iconfont.css">

  

  
    
<link rel="stylesheet" href="/lib/fancybox/fancybox.css">

  

  
    
    
<link rel="stylesheet" href="/lib/highlight/a11y-dark.css">

  

  <script>
    var CONFIG = window.CONFIG || {};
    var ZHAOO = window.ZHAOO || {};
    CONFIG = {
      isHome: false,
      fancybox: true,
      pjax: false,
      lazyload: {
        enable: true,
        only_post: 'false',
        loading: '[object Object]'
      },
      donate: {
        enable: false,
        alipay: 'https://pic.izhaoo.com/alipay.jpg',
        wechat: 'https://pic.izhaoo.com/wechat.jpg'
      },
      galleries: {
        enable: true
      },
      fab: {
        enable: true,
        always_show: false
      },
      carrier: {
        enable: false
      },
      daovoice: {
        enable: false
      },
      preview: {
        background: {
          default: '',
          api: ''
        },
        motto: {
          default: '生于忧患，死于安乐',
          typing: true,
          api: '',
          data_contents: ''
        },
      },
      qrcode: {
        enable: true,
        type: 'url',
        image: 'https://pic.izhaoo.com/weapp-code.jpg',
      },
      toc: {
        enable: true
      },
      scrollbar: {
        type: 'default'
      },
      notification: {
        enable: false,
        delay: 4500,
        list: '',
        page_white_list: '',
        page_black_list: ''
      },
      search: {
        enable: true,
        path: 'search.xml'
      }
    }
  </script>

  

  

<meta name="generator" content="Hexo 6.3.0"></head>

<body class="lock-screen">
  <div class="loading"></div>
  
    


  <nav class="navbar">
    <div class="left">
      
        <i class="iconfont iconhome j-navbar-back-home"></i>
      
      
        <i class="iconfont iconqrcode j-navbar-qrcode"></i>
      
      
        <i class="iconfont iconmoono" id="color-toggle" color-toggle="light"></i>
      
      
        <i class="iconfont iconsearch j-navbar-search"></i>
      
    </div>
    <div class="center">2025 年的 Diffusion 在研究什么</div>
    <div class="right">
      <i class="iconfont iconmenu j-navbar-menu"></i>
    </div>
    
      <div id="qrcode-navbar"></div>
    
  </nav>

  
  

<nav class="menu">
  <div class="menu-container">
    <div class="menu-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <ul class="menu-content"><li class="menu-item">
        <a href="/ " class="underline "> 首页</a>
      </li><li class="menu-item">
        <a href="/categories/ " class="underline "> 分类</a>
      </li></ul>
    
      <div class="menu-copyright"><p>Powered by <a target="_blank" href="https://hexo.io">Hexo</a>  |  Theme - <a target="_blank" href="https://github.com/izhaoo/hexo-theme-zhaoo">zhaoo</a></p></div>
    
  </div>
</nav>
  <main id="main">
  <div class="article-wrap">
    <div class="row container">
      <div class="col-xl-3"></div>
      <div class="col-xl-6"><article class="article">
  <div class="wrap">
    <section class="head">
  <img   class="lazyload" data-original="/images/theme/post.jpg" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="  draggable="false">
  <div class="head-mask">
    <h1 class="head-title">2025 年的 Diffusion 在研究什么</h1>
    <div class="head-info">
      <span class="post-info-item"><i class="iconfont iconcalendar"></i>April 09, 2025</span>
      
      <span class="post-info-item"><i class="iconfont iconfont-size"></i>7815</span>
    </div>
  </div>
</section>
    <section class="main">
      <section class="content">
        <p>好的，我将调研2024-2025年间关于扩散模型（Diffusion Models）的最新学术研究方向，重点关注具有公开GitHub代码的研究成果。<br>请稍等，我现在就开始调研。你可以<strong>刷新页面</strong>来查看进展。</p>
<h1 id="2024–2025年扩散模型最新研究综述"><a href="#2024–2025年扩散模型最新研究综述" class="headerlink" title="2024–2025年扩散模型最新研究综述"></a>2024–2025年扩散模型最新研究综述</h1><p>近年来，<strong>扩散模型</strong>（Diffusion Models）在生成建模领域取得了令人瞩目的成绩，特别是在图像合成等任务上达到了新的水准。2024年至2025年间，围绕扩散模型的学术研究进一步深入，集中在<strong>新方法、新架构和理论进展</strong>等方面。本文将对这一时期的代表性研究方向进行综述，涵盖模型效率提升、条件控制改进、生成质量优化、理论分析突破以及大模型架构升级等内容。我们将介绍每项研究的技术细节和贡献点，并指出其是否开源提供了代码。最后，附上一张汇总表格列出相关研究的名称、作者、来源、发布日期、内容简述和代码链接。</p>
<h2 id="模型效率与采样加速"><a href="#模型效率与采样加速" class="headerlink" title="模型效率与采样加速"></a>模型效率与采样加速</h2><p><strong>扩散模型的高计算量</strong>长期以来限制了其实用性，因此不少研究致力于<strong>提升采样效率、减少采样步数</strong>。2024年起，这一方向涌现出多种新方案：</p>
<ul>
<li><p><strong>潜在一致性模型的高效训练</strong>：Dao等人提出改进的潜在一致性模型训练方法。一致性模型（Consistency Model）是一类可单步生成样本的模型，但直接在大规模<strong>潜在空间</strong>（latent space）训练时表现不佳。该研究通过<strong>Cauchy损失</strong>替换Pseudo-Huber损失以抑制潜在空间中的<strong>离群值</strong>，并在早期扩散步加入额外的扩散损失，利用<strong>最优传输耦合</strong>降低噪声，引入<strong>自适应缩放调度</strong>和<strong>非缩放LayerNorm</strong>等技术，显著提升了一致性模型在潜在空间的训练稳定性。他们成功训练出<strong>一步或两步即可采样高质量图像</strong>的潜在一致性模型，使其生成质量接近常规多步扩散模型。实现代码已开源（项目<strong>sLCT</strong>）。这一工作缩小了一致性模型与扩散模型在潜在空间性能差距，对于<strong>加速文本到图像等大型扩散模型采样</strong>具有重要意义。</p>
</li>
<li><p><strong>上采样扩散概率模型（UDPM）</strong>：Abu-Hussein等人提出“Upsampling Diffusion Probabilistic Model”。不同于标准扩散模型在每一步仅降噪，该方法在<strong>正向过程</strong>中同时对数据进行<strong>降采样和加噪</strong>，即逐步降低分辨率并添加噪声；在<strong>逆过程</strong>则逐步<strong>去噪并上采样</strong>恢复清晰图像。这种设计大幅减少了所需的网络 eval 次数。在实验中，UDPM<strong>仅用3次网络eval</strong>即可生成图像，计算成本总计甚至<strong>低于传统DDPM一次迭代</strong>。它在CIFAR-10等数据集上取得了<strong>FID=6.86</strong>的优异成绩，超越了现有单步生成的高效扩散模型。此外，UDPM提供了<strong>可解释且可插值的潜在空间</strong>，弥补了扩散模型潜在空间难以解释的缺点。作者已在GitHub开源了代码，实现了在FFHQ等数据集上的高保真图像合成。</p>
</li>
<li><p><strong>高阶采样算法与加速理论</strong>：针对扩散模型<strong>采样步骤多、速度慢</strong>的问题，Li等人提出了<strong>无需重新训练</strong>即可加速采样的高阶算法。他们设计了改进的确定性采样器和随机采样器：新的确定性采样器（对应DDIM）收敛速率提升为$O(1/T^2)$，优于原DDIM的$O(1/T)$；改进的随机采样器（对应DDPM）收敛速率达$O(1/T)$，优于原DDPM的$O(1/\sqrt{T})$。这种方法借鉴了<strong>高阶ODE求解器</strong>（如DPM-Solver）的思想，并从理论上证明了采样效率的大幅提升。值得注意的是，该算法<strong>不需要额外训练</strong>，对扩散模型的打分估计误差具有鲁棒性。这一研究为<strong>扩散模型快速采样</strong>提供了坚实的理论支持。</p>
</li>
</ul>
<p>上述方法显著改善了扩散模型的采样效率，使高质量图像的生成可在极少步内完成。这对于扩散模型在实时应用中的部署具有深远意义。</p>
<h2 id="条件控制与多模态指导"><a href="#条件控制与多模态指导" class="headerlink" title="条件控制与多模态指导"></a>条件控制与多模态指导</h2><p>为了<strong>增强扩散模型生成的可控性</strong>，研究者提出了多种<strong>条件控制和引导机制</strong>，使模型能灵活地接受不同模态的条件或关注数据分布的稀有部分。</p>
<ul>
<li><p><strong>通用指导（Universal Guidance）</strong>：Bansal等人在ICLR 2024提出了一种<strong>通用条件引导算法</strong>。传统扩散模型通常针对特定条件（如文本）训练，无法直接应用新条件。该算法允许在<strong>不重新训练扩散模型</strong>的情况下，用任意模态的引导函数来控制生成过程。具体而言，在采样时通过与扩散过程并行的优化迭代，将外部条件（如分割图、人脸识别结果、目标检测框、图像风格或分类器输出等）转化为对噪声预测的附加引导，从而<strong>使预训练扩散模型接受新的条件约束</strong>。实验表明，该方法可成功利用多种指导信号生成相应图像。由于不需重新训练，通用指导具有很强的<strong>泛用性</strong>。作者提供了官方实现，验证了在不同条件下高质量图像生成的效果。</p>
</li>
<li><p><strong>少数样本指导（Minority Guidance）</strong>：Um等人在ICLR 2024提出了一个关注<strong>低密度区域样本</strong>的生成框架。扩散模型倾向于生成数据流形上高密度区域（多数类）的样本，对于<strong>低概率的少数类样本</strong>往往难以覆盖。该工作首先定义了一个衡量样本“独特性”的指标，以识别<strong>数据分布中的少数样本</strong>。然后提出了“少数指导”的采样技术，在扩散采样过程中引入一个<strong>引导项偏向低密度区域</strong>，鼓励模型探索具有目标罕见度的样本。通过这种引导，扩散模型生成<strong>高质量少数样本</strong>的能力大大提高。在医学影像等真实场景中，该方法依然显著提升了模型生成稀有案例的覆盖率和质量。作者已将代码开源，有助于后续研究者在<strong>公平性和多样性</strong>方向拓展扩散模型应用。</p>
</li>
<li><p><strong>跨模态上下文扩散模型（ContextDiff）</strong>：Yang等人在ICLR 2024提出<strong>ContextDiff</strong>框架，旨在<strong>增强文本指导下图像/视频生成的语义对齐</strong>。以往<strong>文本-图像扩散模型</strong>大多只在逆过程融入文本条件，对正向扩散过程的文本相关性考虑不足。这种不一致可能限制文本语义向生成结果的精确传达。为此，ContextDiff在<strong>正向和逆向过程</strong>中都注入跨模态的<strong>文本-视觉上下文信息</strong>，使文本条件在扩散的每个时间步都对噪声演化产生影响。通过对DDPM和DDIM过程的扩展，ContextDiff确保了<strong>扩散轨迹与文本条件的一致性</strong>。在文本到图像生成和文本引导视频编辑任务中，该方法取得了新的<strong>SOTA</strong>表现，大幅提升了生成图像与文本条件的语义一致性。作者提供了代码和模型，证明了在复杂跨模态任务中的有效性。</p>
</li>
</ul>
<p>通过上述改进，扩散模型在<strong>条件可控生成</strong>方面变得更加灵活强大。不仅能适应多种模态的条件约束，还能关注数据中的少数模式和增强跨模态语义对应，从而拓宽了扩散模型在<strong>可控生成和多样性</strong>领域的应用前景。</p>
<h2 id="图像质量提升与训练优化"><a href="#图像质量提升与训练优化" class="headerlink" title="图像质量提升与训练优化"></a>图像质量提升与训练优化</h2><p>除了速度和控制，<strong>提升生成内容的质量</strong>也是研究热点。2024年出现的若干工作聚焦于<strong>改进训练目标</strong>或<strong>生成过程细节</strong>，以获取更高保真的合成图像：</p>
<ul>
<li><p><strong>级联扩散模型结合感知损失</strong>：Jie An等人提出<strong>级联扩散模型（Cas-DM）</strong>，探索将<strong>度量函数（感知损失）引入扩散模型训练</strong>。以LPIPS感知损失为代表的度量在一致性模型中证明有效，但在扩散模型中直接加入会因<strong>训练目标不匹配</strong>而失败。Cas-DM通过<strong>两阶段网络级联</strong>解决了这一问题：第一阶段网络与标准DDPM类似，预测噪声$\epsilon$；第二阶段网络以初步重构的图像$x_0$为输入，输出精细的$x_0$预测，并预测一个动态加权系数用于融合两阶段输出。训练时，对第二阶段预测的$x_0$施加LPIPS损失并<strong>停止梯度回传给第一阶段</strong>，从而既优化图像感知质量又不干扰噪声预测分支。实验证明，Cas-DM成功将LPIPS等感知指标用于扩散模型训练，<strong>在多个基准数据集上取得了最先进的图像质量</strong>（FID、sFID和IS指标显著提升），生成图像的细节和逼真度明显改善。这一方法表明，通过合适的架构设计，<strong>感知级别的度量函数</strong>可以有效提升扩散模型的生成效果。目前论文未公开完整代码，但相关方法为扩散模型融入<strong>感知损失</strong>提供了范式。</p>
</li>
<li><p><strong>频域特征引导的生成优化（DMFFT）</strong>：2025年有研究将<strong>传统傅里叶频域分析</strong>引入扩散模型的生成过程。Wang等人在Scientific Reports发表文章，提出无需额外训练即可提升生成质量的<strong>DMFFT方法</strong>。他们分析了扩散模型U-Net中<strong>上采样阶段</strong>的特征，发现通过调整特征的<strong>频率分量（高频/低频）幅度和相位</strong>可以影响输出图像的质量。具体地，DMFFT在扩散模型上采样的<strong>交叉注意块（CrossAttnUpBlock）</strong>入口处嵌入一个傅里叶变换模块，根据经验调整不同频段的缩放系数，对特征进行频域调制。大量实验表明，该方法能够<strong>显著改善扩散模型生成图像/视频的语义对齐、结构布局、色彩纹理以及时序一致性</strong>，同时提升艺术表现力和多样性，而<strong>无需对扩散模型进行任何额外训练或参数改动</strong>。这意味着只需在推理阶段对特征做频域处理，即可增强输出质量。DMFFT开辟了一个从信号处理角度优化扩散生成的新思路。</p>
</li>
</ul>
<p>这些工作从训练目标和生成过程两个角度出发，提高了扩散模型生成结果的主观和客观质量。例如，Cas-DM通过引入感知损失使生成图像更加清晰自然，DMFFT则通过频域调控增强了图像的细节和一致性。这些改进对<strong>实际应用中获得高保真、无失真的生成结果</strong>非常关键。</p>
<h2 id="理论进展与分析"><a href="#理论进展与分析" class="headerlink" title="理论进展与分析"></a>理论进展与分析</h2><p>随着扩散模型的应用日趋广泛，<strong>理论研究</strong>也在努力解释和提高这些模型的性能。2024年至2025年出现了一系列工作，从<strong>收敛性理论</strong>到<strong>解析扩散过程</strong>等方面取得进展，加深了对扩散模型原理的理解：</p>
<ul>
<li><p><strong>扩散模型收敛率的提升分析</strong>：Li和Jiao等人在2024年针对扩散概率模型（DPM）的收敛性给出了更优的理论保证。此前的研究（如2023年Gupta等）已将扩散采样视作随机过程，并分析了迭代复杂度。Li等人通过引入<strong>随机中点方法</strong>，证明在无对数凹假设下，若采样误差容许$\epsilon$，则扩散模型要达到精度$\epsilon$所需的迭代次数可缩减为$O(d^{1/3}\epsilon^{-2/3})$（其中$d$为数据维度）。这一结果优于先前最佳的$O(d^{5/12}\epsilon^{-1})$复杂度，将依赖维度的阶降低了，并缩小了理论与实践间的差距。该工作无需假设目标分布对数凹，且允许近似的得分估计，更贴近真实扩散模型的条件。此理论进展为理解扩散模型的效率提供了新视角，并证明了<strong>更快采样收敛率</strong>在理论上是可实现的。</p>
</li>
<li><p><strong>确定性采样器的统一收敛分析</strong>：传统的扩散模型理论多依赖随机过程工具（如Girsanov定理）分析随机采样器，对<strong>确定性采样</strong>（如DDIM）缺乏统一理论。Runjia Li等人在2024年提出了一个<strong>统一分析框架</strong>，专门研究<strong>确定性扩散采样器</strong>的收敛性质。他们构建了一般性的分析方法，可适用于各种前向过程和确定性逆过程，弥补了以往只针对特定采样器分析的不足。利用该框架，作者举例分析了<strong>方差保持型前向扩散（VP-SDE）</strong>结合<strong>指数积分采样方案</strong>的情形，证明了$\tilde{O}(d^2/\epsilon)$的迭代复杂度；同时对常用的<strong>DDIM采样器</strong>给出了多项式复杂度的严格证明。这一统一理论为不同类型的扩散采样器提供了<strong>可比的收敛保证</strong>，有助于系统理解各种改进采样算法的性能边界。</p>
</li>
</ul>
<p>上述理论工作为扩散模型提供了更坚实的数学基础。不论是提高收敛速率的算法设计，还是统一解析不同采样过程的框架，都为未来<strong>设计更快更稳健的扩散模型</strong>指明了方向。此外，还有研究将扩散模型视作<strong>策略优化或控制问题</strong>加以分析（如将生成目标融入强化学习奖励框架等），这些也拓宽了扩散模型理论研究的思路。</p>
<h2 id="大模型与架构创新"><a href="#大模型与架构创新" class="headerlink" title="大模型与架构创新"></a>大模型与架构创新</h2><p>在实践层面，2024年前后也出现了<strong>更大规模、更强性能</strong>的扩散模型。<strong>Stable Diffusion XL（SDXL）</strong>是这一时期的代表模型之一：</p>
<ul>
<li><strong>Stable Diffusion XL (SDXL)</strong>：由Stability AI的Podell等人在2023年提出的SDXL模型是<strong>潜在扩散模型（Latent Diffusion）</strong>在高分辨率图像合成上的一次重大升级。与之前的Stable Diffusion版本相比，SDXL的U-Net主干参数量增加了3倍，主要得益于引入<strong>更多的注意力模块</strong>以及<strong>更大的跨模态注意力上下文</strong>。具体来说，SDXL使用了<strong>双文本编码器</strong>（在原有CLIP文本编码器基础上增加第二个文本编码器）来提供更丰富的文本条件语义；设计了多种新的<strong>条件融合机制</strong>，并采用<strong>多长宽比训练</strong>使模型能胜任不同尺寸的图像生成。此外，SDXL还训练了一个<strong>后置精炼模型（refiner）</strong>：先由基础模型生成1024×1024图像，再由精炼模型通过<strong>图像到图像扩散</strong>细化，提高最终输出的视觉保真度。实验结果显示，SDXL相较之前版本的稳定扩散在图像质量上有<strong>显著提升</strong>，在开放评测中达到可与某些封闭源SOTA生成模型媲美的水平。Stability AI已开放了SDXL的代码和模型权重，促进研究者在其基础上进行进一步开发。例如，基于SDXL的模型调优、知识蒸馏等后续研究也陆续出现，使大模型的性能得以在更广泛应用中发挥。</li>
</ul>
<p>SDXL的成功表明，通过<strong>扩大模型规模、改进架构和训练策略</strong>，扩散模型在保持多样性的同时能够生成更高分辨率、更精细的图像。这也为今后<strong>更大、更多模态</strong>的扩散模型（如视频扩散、3D扩散）的研发提供了经验借鉴。</p>
<h2 id="研究汇总表"><a href="#研究汇总表" class="headerlink" title="研究汇总表"></a>研究汇总表</h2><p>如下表格汇总了2024–2025年扩散模型领域的重要研究成果，包括研究名称、主要作者、来源（会议/期刊或预印本）、发布日期、主要贡献简述以及代码链接：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>研究名称</strong></th>
<th><strong>作者</strong></th>
<th><strong>来源</strong></th>
<th><strong>发布日期</strong></th>
<th><strong>研究内容简述</strong></th>
<th><strong>代码链接</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>Stable Diffusion XL (SDXL)</td>
<td>Dustin Podell 等（Stability AI）</td>
<td>arXiv预印本（据ICLR 2024）</td>
<td>2023年7月4日 (v1)</td>
<td>提出大规模<strong>潜在扩散模型</strong>用于高分辨率文本图像生成。U-Net参数扩大3倍，引入<strong>双文本编码器</strong>和新条件机制，支持多尺度、多长宽比训练；另加精炼扩散模型提升细节。生成质量较以往Stable Diffusion大幅提高，开放提供模型权重。</td>
<td><a target="_blank" rel="noopener" href="https://github.com/Stability-AI/generative-models">Stability-AI/generative-models</a> (含SDXL代码和模型)</td>
</tr>
<tr>
<td>改进的一致性模型潜在空间训练 (sLCT)</td>
<td>Quan Dao, Khanh Doan 等</td>
<td>arXiv预印本</td>
<td>2025年2月3日</td>
<td>提出潜在空间下一致性模型的训练改进：使用<strong>Cauchy损失</strong>处理离群值，增加小步扩散损失和<strong>最优传输</strong>变分技术，配合<strong>自适应缩放调度</strong>和非缩放归一化。使一致性模型在VAE潜在空间中可<strong>一到两步生成高质量图像</strong>，性能接近原扩散模型。</td>
<td><a target="_blank" rel="noopener" href="https://github.com/quandao10/sLCT/">quandao10/sLCT</a></td>
</tr>
<tr>
<td>Upsampling Diffusion Probabilistic Model (UDPM)</td>
<td>Shady Abu-Hussein, Raja Giryes</td>
<td>NeurIPS 2024 (Poster)</td>
<td>2024年12月</td>
<td>提出<strong>降采样+加噪/上采样+去噪</strong>的扩散新流程。在正向过程将数据降维，加快逆过程。<strong>仅需3次网络推理</strong>即可生成图像，FID达6.86，优于以往单步生成方法。提供了可解释潜在空间和更高生成效率。</td>
<td><a target="_blank" rel="noopener" href="https://github.com/shadyabh/UDPM/">shadyabh/UDPM</a></td>
</tr>
<tr>
<td>Universal Guidance for Diffusion Models</td>
<td>Arpit Bansal 等</td>
<td>ICLR 2024 (Poster)</td>
<td>2024年1月16日</td>
<td>提出<strong>通用指导算法</strong>，可在<strong>无额外训练</strong>情况下，用任意模态的引导信号控制扩散生成。支持分割、检测、风格等多种条件，同一扩散模型即可多条件通用。方法通过采样阶段优化实现，生成结果质量优秀。</td>
<td><a target="_blank" rel="noopener" href="https://github.com/arpitbansal297/Universal-Guided-Diffusion">arpitbansal297/Universal-Guided-Diffusion</a></td>
</tr>
<tr>
<td>“不偏不倚”：少数样本指导</td>
<td>Soobin Um 等</td>
<td>ICLR 2024 (Poster)</td>
<td>2024年1月16日</td>
<td>提出<strong>少数样本生成框架</strong>，通过定义样本独特性度量和<strong>少数指导</strong>采样技术，引导扩散模型关注数据分布中<strong>低概率区域</strong>。显著提升模型生成<strong>罕见样本</strong>的能力和多样性。在医疗图像等场景下亦有效。</td>
<td><a target="_blank" rel="noopener" href="https://github.com/soobin-um/minority-guidance">soobin-um/minority-guidance</a></td>
</tr>
<tr>
<td>ContextDiff 跨模态上下文扩散</td>
<td>Ling Yang 等</td>
<td>ICLR 2024 (Poster)</td>
<td>2024年1月16日</td>
<td>提出在<strong>正向+逆向扩散过程中融合文本-图像上下文</strong>的模型。通过在所有时间步传播文本条件信息，增强生成结果与文本语义的一致性。用于文本图像生成和文本指导视频编辑均达<strong>SOTA性能</strong>，语义对齐显著改善。</td>
<td><a target="_blank" rel="noopener" href="https://github.com/YangLing0818/ContextDiff">YangLing0818/ContextDiff</a></td>
</tr>
<tr>
<td>Cascaded Diffusion Model (Cas-DM)</td>
<td>Jie An 等（微软）</td>
<td>IJCAI 2024 (Oral)</td>
<td>2024年1月4日</td>
<td>提出<strong>级联两阶段扩散网络</strong>，第一阶段预测噪声，第二阶段预测图像，从而可对第二阶段应用LPIPS等<strong>感知损失</strong>。巧妙避免损失干扰噪声预测，实现感知优化。实现在CIFAR-10、ImageNet等上<strong>FID、IS刷新SOTA</strong>，生成图像伪影更少、细节更佳。</td>
<td>（论文方法证明有效，代码暂未公布）</td>
</tr>
<tr>
<td>DMFFT 频域特征调制方法</td>
<td>Tianrong Wang 等</td>
<td>Scientific Reports</td>
<td>2025年3月25日</td>
<td>提出利用<strong>快速傅里叶变换</strong>调整扩散模型特征频率分量以提升生成质量的方法。在不上线性结构中，对<strong>高低频、幅值和相位</strong>进行缩放，<strong>无需模型训练</strong>即可改善输出的语义一致性、结构和纹理。适用于文本图像和文本视频生成，增强艺术性和多样性。</td>
<td>（方法基于开源Stable Diffusion实现，附于论文）</td>
</tr>
<tr>
<td>加速Score-Based扩散收敛 (Provably Accelerating)</td>
<td>Gen Li 等</td>
<td>arXiv预印本</td>
<td>2024年3月6日</td>
<td>从理论上<strong>加速扩散采样</strong>：设计了<strong>高阶确定性/随机采样器</strong>，将DDIM采样收敛阶从$O(1/T)$提升至$O(1/T^2)$，DDPM采样从$O(1/\sqrt{T})$提升至$O(1/T)$。算法无需额外训练，证明了扩散模型采样的可达更高效率。</td>
<td>（算法伪代码在论文中给出，无单独代码）</td>
</tr>
<tr>
<td>扩散模型收敛率改进分析</td>
<td>Gen Li, Yuchen Jiao</td>
<td>arXiv预印本</td>
<td>2024年10月18日</td>
<td>提出改进的扩散模型收敛复杂度：证明迭代复杂度为$O(d^{1/3}\epsilon^{-2/3})$，优于此前最佳的$O(d^{5/12}\epsilon^{-1})$（$d$为数据维度)。基于随机中点方法的理论分析进一步缩小了理论与实践差距。</td>
<td>（理论研究，无代码）</td>
</tr>
<tr>
<td>确定性扩散采样统一分析</td>
<td>Runjia Li 等</td>
<td>arXiv预印本</td>
<td>2024年10月18日</td>
<td>针对<strong>DDIM等确定性采样</strong>缺乏统一理论的问题，提出通用分析框架。对VP扩散+指数积分方案给出$\tilde{O}(d^2/\epsilon)$复杂度，对DDIM采样证明多项式收敛。为各种扩散采样策略提供了统一的收敛性保证。</td>
<td>（理论研究，无代码）</td>
</tr>
</tbody>
</table>
</div>
<p>上述汇总表中的研究成果体现了2024–2025年扩散模型领域的主要进展。从工程上的模型改进到理论层面的深入分析，这些工作共同推动了扩散模型在<strong>效率、可控性、质量和理解</strong>等方面的提升。展望未来，随着社区持续探索，我们有望见到扩散模型在更多元的任务和更大尺度数据上取得突破，并逐步解决当前存在的速度瓶颈和控制局限，使其在生成AI领域发挥更大的作用。</p>

      </section>
      <section class="extra">
        
          <ul class="copyright">
  
    <li><strong>本文作者：</strong>EnableAsync</li>
    <li><strong>本文链接：</strong><a href="https://enableasync.github.io/uncategorized/Diffusion-2025/index.html" title="https:&#x2F;&#x2F;enableasync.github.io&#x2F;uncategorized&#x2F;Diffusion-2025&#x2F;index.html">https:&#x2F;&#x2F;enableasync.github.io&#x2F;uncategorized&#x2F;Diffusion-2025&#x2F;index.html</a></li>
    <li><strong>版权声明：</strong>本博客所有文章均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" title="BY-NC-SA" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请注明出处！</li>
  
</ul>
        
        
        
  <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/diffusion/" rel="tag">diffusion</a></li></ul> 

        
  <nav class="nav">
    <a href="/uncategorized/leetcode/"><i class="iconfont iconleft"></i>算法整理</a>
    <a href="/uncategorized/mmRadar-Diffusion/">扩散模型在毫米波雷达中的使用综述<i class="iconfont iconright"></i></a>
  </nav>

      </section>
      
    </section>
  </div>
</article></div>
      <div class="col-xl-3">
        
          
  <aside class="toc-wrap">
    <h3 class="toc-title">文章目录：</h3>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2024%E2%80%932025%E5%B9%B4%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%9C%80%E6%96%B0%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0"><span class="toc-text">2024–2025年扩散模型最新研究综述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%95%88%E7%8E%87%E4%B8%8E%E9%87%87%E6%A0%B7%E5%8A%A0%E9%80%9F"><span class="toc-text">模型效率与采样加速</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%A1%E4%BB%B6%E6%8E%A7%E5%88%B6%E4%B8%8E%E5%A4%9A%E6%A8%A1%E6%80%81%E6%8C%87%E5%AF%BC"><span class="toc-text">条件控制与多模态指导</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E6%8F%90%E5%8D%87%E4%B8%8E%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96"><span class="toc-text">图像质量提升与训练优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%AE%BA%E8%BF%9B%E5%B1%95%E4%B8%8E%E5%88%86%E6%9E%90"><span class="toc-text">理论进展与分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%9E%B6%E6%9E%84%E5%88%9B%E6%96%B0"><span class="toc-text">大模型与架构创新</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A0%94%E7%A9%B6%E6%B1%87%E6%80%BB%E8%A1%A8"><span class="toc-text">研究汇总表</span></a></li></ol></li></ol>
  </aside>

        
      </div>
    </div>
  </div>
</main>
  

<footer class="footer">
  <div class="footer-social"><a 
        href="https://github.com/EnableAsync "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#9f7be1'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  icongithub-fill "></i>
      </a></div>
  
    <div class="footer-copyright"><p>Powered by <a target="_blank" href="https://hexo.io">Hexo</a>  |  Theme - <a target="_blank" href="https://github.com/izhaoo/hexo-theme-zhaoo">zhaoo</a></p></div>
  
</footer>
  
      <div class="fab fab-plus">
    <i class="iconfont iconplus"></i>
  </div>
  
  
  <div class="fab fab-up">
    <i class="iconfont iconcaret-up"></i>
  </div>
  
  
  
    
<script src="/js/color-mode.js"></script>

  
  
    <div class="search">
  <div class="search-container">
    <div class="search-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <div class="search-input-wrapper">
      <i class="search-input-icon iconfont iconsearch"></i>
      <input class="search-input" type="search" id="search-input" placeholder="Search..." autofocus autocomplete="off"
        autocorrect="off" autocapitalize="off">
    </div>
    <div class="search-output" id="search-output"></div>
  </div>
</div>
  
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>

<script src="/lib/jquery/jquery.js"></script>



  
<script src="/lib/lazyload/lazyload.js"></script>




  
<script src="/lib/fancybox/fancybox.js"></script>






  
<script src="/lib/qrcode/qrcode.js"></script>




<script src="/js/utils.js"></script>
<script src="/js/script.js"></script>



















</html>